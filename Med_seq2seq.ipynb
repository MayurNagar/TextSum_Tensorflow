{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import string\n",
    "import fileinput\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.models.rnn.translate import seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/media/ai2-rey/data_disk/data_sets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_dir+'abstracts.txt', 'r') as f:\n",
    "    abstracts = [line.strip() for line in f]\n",
    "    abstracts = abstracts[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data_dir+'titles.txt','r') as f:\n",
    "    titles = [line.strip() for line in f]\n",
    "    titles = titles[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = len(abstracts)\n",
    "train_abstracts = abstracts[:int(0.9*l)]\n",
    "train_titles = titles[:int(0.9*l)]\n",
    "\n",
    "test_abstracts = abstracts[int(0.9*l):]\n",
    "test_titles = titles[int(0.9*l):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del abstracts\n",
    "del titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNKNOWN = \"*UNKNOWN*\"\n",
    "PAD = \"*PAD*\"\n",
    "\n",
    "def generate_embedding_files(filename):\n",
    "    embeddings = {}\n",
    "    for line in open(filename):\n",
    "        parts = line.split()\n",
    "        embeddings[parts[0]] = list(map(float, parts[1:]))\n",
    "    embedding_size = len(list(embeddings.values())[0])\n",
    "    embeddings[UNKNOWN] = [0.0 for _ in range(embedding_size)]\n",
    "    embeddings[PAD] = [1.0 for _ in range(embedding_size)]\n",
    "    \n",
    "    words = embeddings.keys()\n",
    "    embedding_matrix = np.array([embeddings[word] for word in list(embeddings.keys())])\n",
    "    return words, embedding_matrix\n",
    "\n",
    "glove_words, glove_embedding_matrix = generate_embedding_files('/media/ai2-rey/data_disk/data_sets/glove.6B/glove.6B.50d.txt')\n",
    "glove_vocab_lookup = {word: i for i, word  in enumerate(glove_words)}\n",
    "glove_vocab_size, glove_embedding_size= glove_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextSum_Dataset:\n",
    "  \n",
    "    def __init__(self, abstract, title, word2id_dict=None, \n",
    "                 id2word_dict=None, use_glove = True, \n",
    "                 max_vocab_size=None, autopad_abstract='max', \n",
    "                 autopad_title ='max', abstract_pad_len=None, title_pad_len = None):\n",
    "\n",
    "        assert autopad_abstract in {'min', 'max', 'avg'}\n",
    "        assert autopad_title in {'min', 'max', 'avg'}\n",
    "     \n",
    "        self.abstract = abstract\n",
    "        self.title = title\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "        self.abstract_pad_len = abstract_pad_len\n",
    "        self.title_pad_len = title_pad_len\n",
    "        self.__autopad_abstract = autopad_abstract\n",
    "        self.__autopad_title = autopad_title\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.word_counter = dict()\n",
    "        self.use_glove = use_glove\n",
    "        self.word2id_dict = word2id_dict\n",
    "        self.id2word_dict = id2word_dict\n",
    "        \n",
    "        self.abstract_ids = None\n",
    "        self.title_ids = None\n",
    "        self.abstract_lens = None\n",
    "        self.titles_lens = None\n",
    "        \n",
    "        self.__parse_data()\n",
    "        if self.word2id_dict is None: self.__create_word2id_dict() \n",
    "        self.__numericize_data()\n",
    "\n",
    "    def __normalize_text(self, s):\n",
    "        def remove_articles(text):\n",
    "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "        def white_space_fix(text):\n",
    "            return ' '.join(text.split())\n",
    "\n",
    "        def remove_punc(text):\n",
    "            exclude = set(string.punctuation)\n",
    "            exclude.discard('.')\n",
    "            return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "        def lower(text):\n",
    "            return text.lower()\n",
    "\n",
    "        def add_space(text):\n",
    "            return text.replace('.', ' . ')\n",
    "\n",
    "        return white_space_fix(add_space(remove_punc(lower(s))))\n",
    "\n",
    "    \n",
    "    def __update_word_counter(self, sequence):\n",
    "        \"\"\" Update word_counter with counts for words in a sentence\n",
    "        \n",
    "        Args:\n",
    "            sequence (list<str>) : list of words in a sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        for word in sequence:\n",
    "            self.word_counter[word] = self.word_counter.get(word, 0) + 1\n",
    "            \n",
    "    def __create_vocab(self):\n",
    "        \"\"\" Create set of most frequent unique words found in the training data \"\"\"\n",
    "        \n",
    "        if self.max_vocab_size == None:\n",
    "            self.vocab = set(self.word_counter.keys())\n",
    "        else:\n",
    "            self.vocab = set(sorted(self.word_counter, key=self.word_counter.get, reverse=True)[:self.max_vocab_size])\n",
    "        \n",
    "    def __shuffle_data(self, data):\n",
    "        random.shuffle(data)\n",
    "        return list(zip(*data))\n",
    "    \n",
    "    def __parse_data(self):\n",
    "        \n",
    "        abstracts = []\n",
    "        titles = []\n",
    "        \n",
    "        for idx in range(len(self.abstract)):\n",
    "            a = self.__normalize_text(self.abstract[idx])\n",
    "            t = self.__normalize_text(self.title[idx])\n",
    "            \n",
    "            self.__update_word_counter(a.split())\n",
    "            self.__update_word_counter(t.split())\n",
    "            \n",
    "            abstracts.append(a)\n",
    "            titles.append(t)\n",
    "            \n",
    "        self.abstract = abstracts\n",
    "        self.title = titles\n",
    "        \n",
    "        del abstracts\n",
    "        del titles\n",
    "        \n",
    "        self.__create_vocab()\n",
    "        \n",
    "        shuffle = list(zip(self.abstract, self.title))        \n",
    "        self.abstract, self.title = self.__shuffle_data(shuffle)\n",
    "    \n",
    "    def __create_word2id_dict(self):\n",
    "        \n",
    "        if self.word2id_dict == None:\n",
    "            self.word2id_dict = dict()\n",
    "            self.id2word_dict = dict()\n",
    "            misc_tokens = ['PAD', 'UNK']\n",
    "\n",
    "            for i, token in enumerate(misc_tokens):\n",
    "                self.word2id_dict[token] = i\n",
    "\n",
    "            for word in self.vocab:\n",
    "                self.word2id_dict[word] = len(self.word2id_dict)\n",
    "\n",
    "            self.vocab |= set(misc_tokens)\n",
    "            \n",
    "            word2id = glove_vocab_lookup if self.use_glove else self.word2id_dict\n",
    "            self.id2word_dict = dict(zip(word2id.values(), word2id.keys()))\n",
    "            \n",
    "            self.num_tokens = len(self.word2id_dict)\n",
    "    \n",
    "    def __convert_word2id(self, word):\n",
    "        \n",
    "        try:\n",
    "            word_id = glove_vocab_lookup[word] if self.use_glove else self.word2id_dict[word]\n",
    "        except:\n",
    "            word_id = glove_vocab_lookup[UNKNOWN] if self.use_glove else self.word2id_dict['UNK']\n",
    "\n",
    "        return word_id\n",
    "    \n",
    "    def __apply_padding(self, s, pad_len):\n",
    "        sequence = s[:]\n",
    "        \n",
    "        PAD_sym = glove_vocab_lookup[PAD] if self.use_glove else self.word2id_dict['PAD'] \n",
    "        \n",
    "        if len(sequence) < pad_len:\n",
    "            sequence += [PAD_sym for i in range(pad_len - len(sequence))]\n",
    "        elif len(sequence) > pad_len:\n",
    "            sequence = sequence[:pad_len]\n",
    "        else:\n",
    "            pass\n",
    "        return sequence\n",
    "        \n",
    "    def __get_seq_length_stats(self, sequences):\n",
    "        max_len = 0\n",
    "        min_len = 100000\n",
    "        avg_len = 0\n",
    "        for sequence in sequences:\n",
    "            max_len = max(max_len, len(sequence))\n",
    "            min_len = min(min_len, len(sequence))\n",
    "            avg_len += len(sequence)\n",
    "        avg_len = int(float(avg_len) / len(sequences))\n",
    "        return min_len, max_len, avg_len\n",
    "\n",
    "    def __get_max_sequence_lengths(self, abstract_ids, title_ids):\n",
    "        min_abstract_len, max_abstract_len, avg_abstract_len = self.__get_seq_length_stats(abstract_ids)\n",
    "        min_title_len, max_title_len, avg_title_len = self.__get_seq_length_stats(title_ids)\n",
    "\n",
    "        if self.abstract_pad_len == None:\n",
    "            if self.__autopad_abstract != None:\n",
    "                if self.__autopad_abstract == 'min':\n",
    "                    self.abstract_pad_len = min_abstract_len\n",
    "                elif self.__autopad_abstract == 'max':\n",
    "                    self.abstract_pad_len = max_abstract_len\n",
    "                elif self.__autopad_abstract == 'avg':\n",
    "                    self.abstract_pad_len = avg_abstract_len\n",
    "            else:\n",
    "                self.abstract_pad_len = avg_abstract_len\n",
    "                \n",
    "        if self.title_pad_len == None:\n",
    "            if self.__autopad_title != None:\n",
    "                if self.__autopad_title == 'min':\n",
    "                    self.title_pad_len = min_title_len\n",
    "                elif self.__autopad_title == 'max':\n",
    "                    self.title_pad_len = max_title_len\n",
    "                elif self.__autopad_title == 'avg':\n",
    "                    self.title_pad_len = avg_title_len\n",
    "            else:\n",
    "                self.title_pad_len = avg_title_len     \n",
    "    \n",
    "    def __tokenize_sentences(self, abstract_ids, title_ids):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        abstract_tokens = []\n",
    "        title_tokens = []\n",
    "        \n",
    "        for i in range(len(abstract_ids)):\n",
    "            a_tkn_ids = self.__apply_padding(abstract_ids[i], self.abstract_pad_len)\n",
    "            abstract_tokens.append(a_tkn_ids)\n",
    "            t_tkn_ids = self.__apply_padding(title_ids[i], self.title_pad_len)\n",
    "            title_tokens.append(t_tkn_ids)\n",
    "            \n",
    "        return abstract_tokens, title_tokens\n",
    "        \n",
    "    def __convert_text2ids(self, abstract, title):\n",
    "        \"\"\" Tokenizes sentences.\n",
    "        :param raw: dict returned from load_babi\n",
    "        :param word_table: WordTable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        abstract_ids = []\n",
    "        title_ids = []\n",
    "        \n",
    "        for i in range(len(abstract)):\n",
    "            a_ids = [self.__convert_word2id(word) for word in abstract[i].split()]\n",
    "            abstract_ids.append(a_ids)\n",
    "            t_ids = [self.__convert_word2id(word) for word in title[i].split()]\n",
    "            title_ids.append(t_ids) \n",
    "        \n",
    "        return abstract_ids, title_ids\n",
    "          \n",
    "    \n",
    "    def __numericize_data(self):\n",
    "        a, t = self.__convert_text2ids(self.abstract, self.title)\n",
    "        \n",
    "        self.abstract_lens = [len(i) for i in a]\n",
    "        self.titles_lens = [len(i) for i in t]\n",
    "        \n",
    "        self.__get_max_sequence_lengths(a,t)\n",
    "        \n",
    "        self.abstract_ids, self.title_ids = self.__tokenize_sentences(a,t)\n",
    "        del a,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = TextSum_Dataset(train_abstracts, train_titles, max_vocab_size=50000, abstract_pad_len=200, title_pad_len=20, use_glove=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = TextSum_Dataset(test_abstracts, test_titles, train.word2id_dict, train.id2word_dict, \n",
    "                      abstract_pad_len = train.abstract_pad_len, title_pad_len = train.title_pad_len, use_glove=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.word2id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('training_vars.pickle', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([train.word2id_dict, train.id2word_dict, train.vocab, train.abstract_pad_len, train.title_pad_len], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_abstracts, train_titles, test_abstracts, test_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:  \n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iterator = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.data_iterator)\n",
    "        except StopIteration:\n",
    "            self.data_iterator = self.make_random_iter()\n",
    "            idxs = next(self.data_iterator)\n",
    "            \n",
    "        batch = [self.data[i] for i in idxs]\n",
    "        batch_idxs = [idx for idx in idxs]\n",
    "        return batch, batch_idxs\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids = list(zip(train.abstract_ids, train.title_ids))\n",
    "test_ids = list(zip(test.abstract_ids, test.title_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iter = DataIterator(train_ids,128)\n",
    "test_data_iter = DataIterator(test_ids,128)\n",
    "deploy_data_iter = DataIterator(train_ids,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_dir():\n",
    "    model_dir = os.getcwd() + '/'\n",
    "    if not os.path.exists(model_dir + 'weights'):\n",
    "        os.makedirs(model_dir + 'weights')\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_log(string,filename):\n",
    "#     print(string)\n",
    "    with open(filename,'a') as write_file:\n",
    "        write_file.write(string + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_function(prev, embedding, w, b, update_embedding=True):\n",
    "    prev = tf.nn.xw_plus_b(prev, w, b)\n",
    "    prev_symbol = tf.argmax(prev,1) #maybe 0 if not transpose\n",
    "    emb_prev = tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "    if not update_embedding:\n",
    "        emb_prev = tf.stop_gradient(emb_prev)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract_max_len = train.abstract_pad_len\n",
    "title_max_len = train.title_pad_len\n",
    "\n",
    "n_hidden = 150 \n",
    "n_hidden_layers = 2\n",
    "word_dim = glove_embedding_size\n",
    "vocab_size = len(train.vocab)\n",
    "\n",
    "learning_rate = 0.001\n",
    "train_iters = 100000\n",
    "keep_prob = 0.5\n",
    "\n",
    "display_step = 200\n",
    "val_interval = 1000\n",
    "save_weights_interval = 1000\n",
    "deploy_interval = 1000\n",
    "\n",
    "\n",
    "device = '/gpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dir = prepare_model_dir()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "passage = tf.placeholder(tf.int32,[None, abstract_max_len])\n",
    "#passage_lens = tf.placeholder(tf.int32,[None])\n",
    "summary = tf.placeholder(tf.int64,[None, title_max_len])\n",
    "\n",
    "dropout = tf.placeholder(tf.float32)\n",
    "batch_size = tf.placeholder(tf.int32)\n",
    "use_prev = tf.placeholder(tf.bool)\n",
    "\n",
    "encoder_inputs = tf.unstack(passage, axis=1)\n",
    "labels = tf.unstack(summary, axis=1)\n",
    "decoder_inputs = [tf.zeros_like(labels[0], dtype=tf.int64, name='GO')] + labels[:-1]\n",
    "\n",
    "with tf.variable_scope('embedding'):\n",
    "    embedding = tf.Variable(tf.truncated_normal(shape=[vocab_size, word_dim], stddev=1e-4))\n",
    "    #embedding = tf.constant(glove_embedding_matrix, name='embeddings', dtype=tf.float32)\n",
    "    \n",
    "    emb_enc_inputs = [tf.nn.embedding_lookup(embedding, x)\n",
    "                      for x in encoder_inputs]\n",
    "    emb_dec_inputs = [tf.nn.embedding_lookup(embedding, x)\n",
    "                      for x in decoder_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('encoder'):\n",
    "    enc_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=dropout)\n",
    "    #enc_cell = tf.nn.rnn_cell.MultiRNNCell([enc_cell]*n_hidden_layers)\n",
    "\n",
    "    enc_state = enc_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    h_states = []\n",
    "    c_states = []\n",
    "    \n",
    "    for i in range(abstract_max_len):\n",
    "        h, enc_state = enc_cell(emb_enc_inputs[i], enc_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "#         h_states.append(enc_state.h)\n",
    "#         c_states.append(enc_state.c)\n",
    "    \n",
    "#     h_states = tf.transpose(tf.stack(h_states),[1,0,2])\n",
    "#     h_indices = tf.range(tf.shape(h_states)[0])*tf.shape(h_states)[1]+(passage_lens-1)\n",
    "#     h_thought = tf.gather(tf.reshape(h_states,[-1,n_hidden]), h_indices)\n",
    "    \n",
    "#     c_states = tf.transpose(tf.stack(c_states),[1,0,2])\n",
    "#     c_indices = tf.range(tf.shape(c_states)[0])*tf.shape(c_states)[1]+(passage_lens-1)\n",
    "#     c_thought = tf.gather(tf.reshape(c_states,[-1,n_hidden]), c_indices)\n",
    "\n",
    "w = tf.Variable(tf.truncated_normal(shape=[n_hidden, vocab_size], stddev=1e-4))\n",
    "w_t = tf.transpose(w)\n",
    "b = tf.Variable(tf.truncated_normal(shape=[vocab_size], stddev=1e-4))\n",
    "\n",
    "with tf.variable_scope('decoder'):\n",
    "\n",
    "    dec_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=dropout)\n",
    "    #dec_cell = tf.nn.rnn_cell.OutputProjectionWrapper(dec_cell, vocab_size)\n",
    "\n",
    "    dec_state =  enc_state\n",
    "\n",
    "    dec_h_states = []\n",
    "    \n",
    "    for i in range(title_max_len):\n",
    "        if use_prev == True and i>0:\n",
    "            prev_ids = tf.argmax(tf.nn.softmax(tf.matmul(h, w) + b), axis=1)\n",
    "            prev_word = tf.nn.embedding_lookup(embedding, prev_ids)\n",
    "        else: \n",
    "            prev_word = emb_dec_inputs[i]\n",
    "\n",
    "        h, dec_state = dec_cell(prev_word, dec_state)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        dec_h_states.append(h)\n",
    "\n",
    "with tf.variable_scope('dense_output'):\n",
    "    generated_tokens = []\n",
    "    output_logits = []\n",
    "    \n",
    "    for h_state in dec_h_states:\n",
    "        logits = tf.matmul(h_state, w) + b\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        pred_ids = tf.argmax(probs, axis=1)\n",
    "\n",
    "        output_logits.append(logits)\n",
    "        generated_tokens.append(pred_ids)\n",
    "\n",
    "with tf.variable_scope('sampled_loss'):\n",
    "    labels = labels\n",
    "    decoder_loss = 0.0\n",
    "\n",
    "    for i, logits in enumerate(output_logits):\n",
    "        step_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels[i])\n",
    "        decoder_loss += tf.reduce_mean(step_loss)\n",
    "    loss = decoder_loss / float(title_max_len)\n",
    "\n",
    "with tf.variable_scope('accuracy'):\n",
    "    labels = decoder_inputs\n",
    "    accuracy = 0\n",
    "\n",
    "    for i, token_id in enumerate(generated_tokens):\n",
    "        accuracy += tf.reduce_mean(tf.cast(tf.equal(token_id, labels[i]), tf.float32))\n",
    "    accuracy = accuracy / float(title_max_len)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_file = model_dir + 'seq2seq_log.txt'\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "log=open(log_file,'w')\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, accuracy = 0.2512, loss = 4.643216\n"
     ]
    }
   ],
   "source": [
    "for train_iter in range(train_iters):\n",
    "    train_iter +=1\n",
    "    train_batch, _ = train_data_iter.next_batch()\n",
    "    train_abstract, train_title = zip(*train_batch)\n",
    "    if train_iter==1:\n",
    "        init_feed_dict = {passage:train_abstract,\n",
    "                          summary: train_title,\n",
    "                          dropout: 1.0,\n",
    "                          batch_size: train_data_iter.batch_size,\n",
    "                          use_prev: False}\n",
    "        \n",
    "        init_loss, init_acc = tuple(sess.run([loss, accuracy], \n",
    "                                            feed_dict=init_feed_dict))\n",
    "        log_output = (\"Iter {}, accuracy = {:.4f}, loss = {:.6f}\").format(train_iter, init_acc, init_loss)\n",
    "        \n",
    "        print(log_output)\n",
    "        write_to_log(log_output, log_file)\n",
    "    \n",
    "    train_feed_dict = {passage:train_abstract, \n",
    "                       summary: train_title,\n",
    "                       dropout: keep_prob,\n",
    "                       batch_size: train_data_iter.batch_size,\n",
    "                       use_prev: False}\n",
    "    \n",
    "    sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "    \n",
    "    if train_iter % display_step ==0:\n",
    "        disp_feed_dict = {passage:train_abstract,\n",
    "                          summary: train_title,\n",
    "                          dropout: 1.0,\n",
    "                          batch_size: train_data_iter.batch_size,\n",
    "                          use_prev: False\n",
    "                         }\n",
    "        \n",
    "        train_loss, train_acc = tuple(sess.run([loss, accuracy], \n",
    "                                               feed_dict=disp_feed_dict))\n",
    "        \n",
    "        log_output = (\"Iter {}, accuracy = {:.4f}, loss = {:.6f}\").format(train_iter, train_acc, train_loss)\n",
    "        print(log_output)\n",
    "        write_to_log(log_output, log_file)\n",
    "    \n",
    "    if test_data_iter !=None and train_iter% val_interval==0:\n",
    "        test_batch, _ = test_data_iter.next_batch()\n",
    "        test_abstract, test_title = zip(*test_batch)\n",
    "        test_feed_dict = {passage:test_abstract,\n",
    "                          summary: test_title,\n",
    "                          dropout: 1.0,\n",
    "                          batch_size: test_data_iter.batch_size,\n",
    "                          use_prev: True}\n",
    "        \n",
    "        test_loss, test_acc = tuple(sess.run([loss, accuracy], \n",
    "                                             feed_dict=test_feed_dict))\n",
    "        \n",
    "        log_output = (\"Iter {}, VALIDATION -- accuracy = {:.4f}, loss = {:.6f}\").format(train_iter, test_acc, test_loss)\n",
    "        print(log_output)\n",
    "        write_to_log(log_output, log_file)\n",
    "    \n",
    "    if deploy_data_iter !=None and train_iter % deploy_interval==0:\n",
    "        deploy_batch, _ = deploy_data_iter.next_batch()\n",
    "        deploy_abstract, deploy_title = zip(*deploy_batch)\n",
    "        \n",
    "        model_pred = sess.run(generated_tokens, feed_dict={passage: deploy_abstract,\n",
    "                                                           summary: deploy_title,\n",
    "                                                           dropout: 1.0,\n",
    "                                                           batch_size: deploy_data_iter.batch_size, \n",
    "                                                           use_prev: True})\n",
    "        \n",
    "        label_ids = deploy_title[0]\n",
    "        pred_ids = [x[0] for x in model_pred]\n",
    "        log_output = ''\n",
    "        if train.id2word_dict != None:\n",
    "            log_output = ('PASSAGE\\n{}\\nModel Pred = {}\\nGround Truth = {}\\n'\n",
    "                          .format(' '.join([train.id2word_dict[idx] for idx in deploy_abstract[0]]),\n",
    "                                  ' '.join([train.id2word_dict[idx] for idx in pred_ids]),\n",
    "                                  ' '.join([train.id2word_dict[idx] for idx in label_ids]))\n",
    "                         )\n",
    "        else:\n",
    "            log_output = ('Model Pred = {}, Ground Truth = {}'\n",
    "                          .format(pred_ids, label_ids)\n",
    "                         )\n",
    "        print(log_output)\n",
    "        write_to_log(log_output, log_file)\n",
    "    \n",
    "    if train_iter% save_weights_interval ==0:\n",
    "        weights_dir = model_dir + \"weights/seq2seq_weights_iter--{}.ckpt\".format(train_iter)\n",
    "        save_path = saver.save(sess,weights_dir)\n",
    "        save_string = \"Model saved in file: {}\".format(save_path)\n",
    "        print(save_string)\n",
    "        write_to_log(save_string, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
