{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "import operator\n",
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import string\n",
    "import fileinput\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('training_vars.pickle', 'rb') as f: \n",
    "    word2id_dict, id2word_dict, vocab, abstract_pad_len, title_pad_len = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        exclude.discard('.')\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def add_space(text):\n",
    "        return text.replace('.', ' . ')\n",
    "\n",
    "    return white_space_fix(add_space(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_word2id(word):\n",
    "\n",
    "    try:\n",
    "        word_id = word2id_dict[word]\n",
    "    except:\n",
    "        word_id = word2id_dict['UNK']\n",
    "\n",
    "    return word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_padding(s, pad_len):\n",
    "    sequence = s[:]\n",
    "\n",
    "    PAD_sym = word2id_dict['PAD'] \n",
    "\n",
    "    if len(sequence) < pad_len:\n",
    "        sequence += [PAD_sym for i in range(pad_len - len(sequence))]\n",
    "    elif len(sequence) > pad_len:\n",
    "        sequence = sequence[:pad_len]\n",
    "    else:\n",
    "        pass\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_seq(s, pad_len):\n",
    "    return apply_padding([convert_word2id(i) for i in normalize_text(s).split()], pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(n_hidden, word_dim, learning_rate, gpu_device=0):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    gpu_device_name = '/gpu:{}'.format(gpu_device)\n",
    "    \n",
    "    with graph.as_default(), tf.device(gpu_device_name):\n",
    "        \n",
    "        global passage \n",
    "        passage = tf.placeholder(tf.int32,[None, abstract_pad_len])\n",
    "        global summary \n",
    "        summary = tf.placeholder(tf.int64,[None, title_pad_len])\n",
    "        \n",
    "        global dropout \n",
    "        dropout = tf.placeholder(tf.float32)\n",
    "        global batch_size\n",
    "        batch_size = tf.placeholder(tf.int32)\n",
    "        global use_prev \n",
    "        use_prev = tf.placeholder(tf.bool)\n",
    "\n",
    "        encoder_inputs = tf.unstack(passage, axis=1)\n",
    "        decoder_inputs = tf.unstack(summary, axis=1)\n",
    "        decoder_inputs = [tf.zeros_like(encoder_inputs[0], dtype=tf.int64, name='GO')] + decoder_inputs[:-1]\n",
    "        \n",
    "        with tf.variable_scope('embedding'):\n",
    "            embedding = tf.Variable(tf.truncated_normal(shape=[vocab_size, word_dim], stddev=1e-4))\n",
    "            #embedding = tf.constant(glove_embedding_matrix, name='embeddings', dtype=tf.float32)\n",
    "\n",
    "            emb_enc_inputs = [tf.nn.embedding_lookup(embedding, x)\n",
    "                              for x in encoder_inputs]\n",
    "            emb_dec_inputs = [tf.nn.embedding_lookup(embedding, x)\n",
    "                              for x in decoder_inputs]  \n",
    "        \n",
    "        with tf.variable_scope('encoder'):\n",
    "            enc_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "            enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=dropout)\n",
    "            #enc_cell = tf.nn.rnn_cell.MultiRNNCell([enc_cell]*n_hidden_layers)\n",
    "\n",
    "            enc_state = enc_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "            for i in range(abstract_pad_len):\n",
    "                h, enc_state = enc_cell(emb_enc_inputs[i], enc_state)\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "        w = tf.Variable(tf.truncated_normal(shape=[n_hidden, vocab_size], stddev=1e-4))\n",
    "        w_t = tf.transpose(w)\n",
    "        b = tf.Variable(tf.truncated_normal(shape=[vocab_size], stddev=1e-4))\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "\n",
    "            dec_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "            dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=dropout)\n",
    "            #dec_cell = tf.nn.rnn_cell.OutputProjectionWrapper(dec_cell, vocab_size)\n",
    "\n",
    "            dec_state =  enc_state\n",
    "            \n",
    "            dec_h_states = []\n",
    "            \n",
    "            for i in range(title_pad_len):\n",
    "                if use_prev == True and i>0:\n",
    "                    prev_ids = tf.argmax(tf.nn.softmax(tf.matmul(h, w) + b), axis=1)\n",
    "                    prev_word = tf.nn.embedding_lookup(embedding, prev_ids)\n",
    "                else: \n",
    "                    prev_word = tf.zeros_like(emb_enc_inputs[0], dtype=tf.float32, name='GO')\n",
    "\n",
    "                h, dec_state = dec_cell(prev_word, dec_state)\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                dec_h_states.append(h)\n",
    "                    \n",
    "        with tf.variable_scope('dense_output'):\n",
    "            global generated_tokens \n",
    "            generated_tokens = []\n",
    "            output_logits = []\n",
    "            \n",
    "            for h in dec_h_states:\n",
    "                logits = tf.matmul(h, w) + b\n",
    "                probs = tf.nn.softmax(logits)\n",
    "                pred_ids = tf.argmax(probs, axis=1)\n",
    "                \n",
    "                output_logits.append(logits)\n",
    "                generated_tokens.append(pred_ids)\n",
    "        \n",
    "        with tf.variable_scope('sampled_loss'):\n",
    "            labels = decoder_inputs\n",
    "            decoder_loss = 0.0\n",
    "            \n",
    "            for i, logits in enumerate(output_logits):\n",
    "                step_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels[i])\n",
    "                decoder_loss += tf.reduce_mean(step_loss)\n",
    "            loss = decoder_loss / float(title_pad_len)\n",
    "        \n",
    "        with tf.variable_scope('accuracy'):\n",
    "            labels = decoder_inputs\n",
    "            accuracy = 0\n",
    "            \n",
    "            for i, token_id in enumerate(generated_tokens):\n",
    "                accuracy += tf.reduce_mean(tf.cast(tf.equal(token_id, labels[i]), tf.float32))\n",
    "            accuracy = accuracy / float(title_pad_len)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        \n",
    "        global saver \n",
    "        saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_session(sess):\n",
    "    \n",
    "    n_hidden = 150\n",
    "    word_dims = 50\n",
    "    learning_rate = 0.001\n",
    "    gpu_device = 0\n",
    "    \n",
    "    build_model(n_hidden, word_dims, learning_rate, gpu_device)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver.restore(sess, 'weights/seq2seq_weights_iter--4000.ckpt')\n",
    "    return sess  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode_line(sess, sentence):\n",
    "    \n",
    "    token_ids = tokenize_seq(sentence, abstract_pad_len)\n",
    "    \n",
    "    dep_feed_dict = {passage:[token_ids],\n",
    "                     dropout: 1.0,\n",
    "                     batch_size: 1,\n",
    "                     use_prev: True\n",
    "                    }\n",
    "    logits = sess.run(generated_tokens, feed_dict = dep_feed_dict) \n",
    "    print(logits)\n",
    "    return \" \".join([id2word_dict[i[0]] for i in logits])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
